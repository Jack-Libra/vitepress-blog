# 🧠 從腳本爬蟲到模組化資料流：用 n8n + crawl4ai 打造電商分析平台
📌 TL;DR：這是偏向於技術實戰導向的文章。我將聚焦於如何以 n8n 搭配 crawl4ai 建構出一套模組化、自動化的電商資料分析平台，並分享我為什麼選擇這套組合。內容將包含n8n中每個節點的配置、流程邏輯、錯誤處理、資料清洗、排程執行與跨平台串接等。


## ✅ 為什麼選擇 n8n + crawl4ai？
|     工具     |                          優勢                          |
|:------------:|:------------------------------------------------------:|
|   **n8n**    |    視覺化流程設計、自動排程、易除錯、串接 DB / API     |
| **crawl4ai** | 基於 CSS selector、YAML 設定、穩定輕巧、快速擴展多網站 |

## 個人因素
- **「自動化」更符合爬蟲本質** *(我認為最重要的)*
- 完全開源且免費的工具
- 可擴充性與優秀的效能表現


## 🛠️ N8N 自動化流程設計解析
### 流程總覽
![螢幕擷取畫面 2025-07-28 174209](/images/第二篇/N8N流程總覽.png)
## 1️⃣請求任務 crawl post
```
(1)  不同部署環境下，URL 的設定方式也會不同
Docker Compose(專案default) http://crawl4ai:11235/crawl
n8n、crawl4ai 跑本機 http://localhost:11235/crawl
n8n、crawl4ai 跑本機docker容器 http://host.docker.internal:11235/crawl
雲端主機 https://your-domain.com/crawl

(2) credential
Bearer CRAWL4AI_API_TOKEN
CRAWL4AI_API_TOKEN 於 docker-compose設定，default:0000

(3) 爬蟲 URL
開啟欲爬取網頁devtools以重新設定該網頁css
**示範網頁:https://www.amazon.com/-/zh_TW/gp/bestsellers/electronics/ref=pd_zg_ts_electronics** 
**可透過sitemap獲得大量欲爬取網頁**

(4) URL設定對應 crawl post
Docker Compose(專案default) http://crawl4ai:11235/task/taskID
n8n、crawl4ai 跑本機 http://localhost:11235/task/taskID
n8n、crawl4ai 跑本機docker容器 http://host.docker.internal:11235/task/taskID
雲端主機 https://your-domain.com/task/taskID
```
<table>
	<tr>
	    <td align="center">流程</td>
	    <td align="center">節點</td>
	    <td align="center">說明</td>
	</tr >
    <tr>
        <th rowspan="3">透過專屬 API獲取資料</th>
        <td align="center">
            <img src="/images/第二篇/1-1.png"/>HTTP Request
        </td>
        <td align="center">向crawl4ai POST/GET 任務，crawl4ai將返回任務ID</td>
    </tr>
    <tr>
         <td align="center">
            <img src="/images/第二篇/1-2.png"/>HTTP Request
        </td>
         <td align="center">依據任務ID GET 結果</td>
    </tr>
    <tr>
	     <td align="center">
            <img src="/images/第二篇/1-3.png"/>EDIT FIELDS+IF
        </td>
         <td align="center">判斷任務是否成功</td>
	</tr>
</table>



## 2️⃣資料清洗
<table>
	<tr>
	    <td align="center">流程</td>
	    <td align="center">節點</td>
	    <td align="center">說明</td>
	</tr >
    <tr>
        <th rowspan="2">資料清洗、欄位整理</th>
        <td align="center">
            <img src="/images/第二篇/2-1.png"/>CODE
        </td>
        <td >將爬取資料做清洗 例:[a,b,1,2,A,B]→[a,b]、[1,2]、[A,B]</td>
    </tr>
    <tr>
         <td align="center">
            <img src="/images/第二篇/2-2.png" />      SPLIT OUT
        </td>
         <td>分配資料屬性，1 item=1 product 例:[a,b]、[1,2]、[A,B]→[a,1,A]、[b,2,B]</td>
    </tr>
</table>


## 3️⃣存進資料庫
```
(1) HTTP Request節點:請求查看products表是否有重複ID
URL:https://<your-project-id>.supabase.co/rest/v1/products?product_code=eq.productID

(2) HTTP Request credentials:
apikey : your-anon-key
Authorization : Bearer your-anon-key

(3) supabase credentials
Host : NEXT_PUBLIC_SUPABASE_URL(https://your_account.supabase.co)
Service Role Secret : Bearer service_role API keys
```
<table>
	<tr>
	    <td align="center">流程</td>
	    <td align="center">節點</td>
	    <td align="center">說明</td>
	</tr >
    <tr>
        <th rowspan="3">呼叫SUPABASE API</th>
        <td align="center">
            <img src="/images/第二篇/3-1.png"/>Loop Over Items
        </td>
        <td align="center">一次處裡一筆商品</td>
    </tr>
    <tr>
         <td align="center">
            <img src="/images/第二篇/3-2.png" />      HTTP Request
        </td>
         <td align="center">根據productID查訊SUPABASE資料庫</td>
    </tr>
    <tr>
         <td align="center">
            <img src="/images/第二篇/3-3.png" />      IF
        </td>
         <td align="center">根據結果判斷有無商品資料</td>
    </tr>
    <tr>
        <th rowspan="2">存入資料庫</th>
        <td align="center">
            <img src="/images/第二篇/3-4.png"/>SUPABASE
        </td>
        <td align="center">如果資料庫沒商品資料，則先存入product table，再進snapshot table。</td>
    </tr>
    <tr>
         <td align="center">
            <img src="/images/第二篇/3-5.png" />      SUPABASE
        </td>
         <td align="center">如果資料庫有資料，直接進入snapshot table，記錄此次爬蟲結果。</td>
    </tr>
</table>


## 0️⃣ crawl4ai 請求格式(css strategy)
```
{
  "urls": ["https://www.amazon.com/-/zh_TW/gp/bestsellers/electronics/ref=pd_zg_ts_electronics"], 
"crawler_params": {
  "headless": true,
  "wait_before_extract": 3000},
  "extraction_config": {
    "type": "json_css",
    "params": {
      "schema": {
        "name": "character",
        "baseSelector": "div.p13n-desktop-grid",
        "fields": [
              {
                "name": "Name",
                "selector": "._cDEzb_p13n-sc-css-line-clamp-3_g3dy1",
                "type": "list",
                "fields":[{"name": "Name","type": "text"}]
               },
              {
                "name": "AsinList",
                "selector": "._cDEzb_iveVideoWrapper_JJ34T",
                "type": "list",
                "fields":[{"name": "asin",
      "type": "attribute",
      "attribute": "data-asin"}]
               },
              {
                "name": "Rank",
                "selector": "span.zg-bdg-text",
                "type": "list",
                "fields":[{"name": "Name","type": "text"}]
               },
              {
                "name": "Rate",
                "selector": ".a-icon-row",
                "type": "list",
                "fields":[{"name": "Name","type": "text"}]
               },
              {
                "name": "Price",
                "selector": "span.p13n-sc-price, span._cDEzb_p13n-sc-price_3mJ9Z",
                "type": "list",
                "fields":[{"name": "Name","type": "text"}]
               }
                    ]
      },
      "verbose": true
    }
  },
  "cache_mode": "bypass",
  "semphore_count": 1,

"delay_between_requests": 3000  
}
```


## 🔍 系統觀測與錯誤處理機制
- ✅ 系統監控
- 📤 成功與失敗紀錄推送 Slack
- 🔍 JSON Schema 驗證資料完整性
- 📈 商品數、價格異常比例每日統計
- 🧪 Debug 模式顯示原始 JSON 結果

## ❌ 常見錯誤與解法
|        問題         |                       解法                       |
|:-------------------:|:------------------------------------------------:|
| Selector 抓不到資料 | DevTools 手動測試，避免使用動態產生的 class 名稱 |
|  JSON 欄位型別錯誤  |  Function Node 加入型別強制轉換 (如 \`Number(x)  |
|  Supabase 寫入失敗  |       檢查欄位命名、大小寫一致性與格式設定       |
|  n8n 無法解析結構   |        用 `item[0].json` 路徑處理多層結構        |


## 📊 自動化帶來的效益
|       指標       |       手動方式       | 自動流程（n8n + crawl4ai）  |
|:----------------:|:--------------------:|:---------------------------:|
|   資料更新頻率   | 偶爾，取決於人力時間 |    每日自動，時間可配置     |
| 擴充網站資料來源 |      要寫新程式      | 僅新增 selector YAML + 節點 |
| 資料錯誤偵測能力 |      靠人工檢查      |    Slack 通知 + 錯誤記錄    |
|  多平台部署能力  |        難整合        |   模組化架構支援獨立部署    |

## 🚀 可擴充應用規劃
✅ 商品價格歷史紀錄與比價趨勢

✅ 每日/每週報表自動生成（CSV / PDF）

✅ 結合 GPT 對商品進行摘要與分群

✅ 結合 Telegram / LINE Bot 用戶互動查詢

✅ 提供公開 API 供他人串接熱門商品排行

## 🧠 結語｜爬蟲不是目的，資料流才是價值
從一隻腳本爬蟲轉向一條自動化資料管線，我學到的不只是技術，更是如何設計一套穩定、可觀測、好擴充的資料處理架構。

如果你也想：

- 擺脫手動執行爬蟲腳本的日常

- 提升爬蟲穩定性與資料流程整合

- 打造可延伸的分析系統

那麼：n8n + crawl4ai 是很值得嘗試的組合。

📢 想看更多 n8n 或電商爬蟲實戰？  
📎 歡迎追蹤我的 Medium / GitHub 👉 [@Jack-Libra](https://github.com/Jack-Libra)  
💬 如果你對這個專案有任何建議或想法，歡迎留言交流！
